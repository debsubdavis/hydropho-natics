{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Cvf2EYL3nAJ"
      },
      "outputs": [],
      "source": [
        "# Import pacakages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to calculate Intersection over Union (IoU)\n",
        "def iou(box1, box2, is_pred=True):\n",
        "\tif is_pred:\n",
        "\t\t# IoU score for prediction and label\n",
        "\t\t# box1 (prediction) and box2 (label) are both in [x, y, width, height] format\n",
        "\n",
        "\t\t# Box coordinates of prediction\n",
        "\t\tb1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
        "\t\tb1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
        "\t\tb1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
        "\t\tb1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
        "\n",
        "\t\t# Box coordinates of ground truth\n",
        "\t\tb2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
        "\t\tb2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
        "\t\tb2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
        "\t\tb2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
        "\n",
        "\t\t# Get the coordinates of the intersection rectangle\n",
        "\t\tx1 = torch.max(b1_x1, b2_x1)\n",
        "\t\ty1 = torch.max(b1_y1, b2_y1)\n",
        "\t\tx2 = torch.min(b1_x2, b2_x2)\n",
        "\t\ty2 = torch.min(b1_y2, b2_y2)\n",
        "\t\t# Make sure the intersection is at least 0\n",
        "\t\tintersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "\t\t# Calculate the union area\n",
        "\t\tbox1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n",
        "\t\tbox2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n",
        "\t\tunion = box1_area + box2_area - intersection\n",
        "\n",
        "\t\t# Calculate the IoU score\n",
        "\t\tepsilon = 1e-6\n",
        "\t\tiou_score = intersection / (union + epsilon)\n",
        "\n",
        "\t\t# Return IoU score\n",
        "\t\treturn iou_score\n",
        "\n",
        "\telse:\n",
        "\t\t# IoU score based on width and height of bounding boxes\n",
        "\n",
        "\t\t# Calculate intersection area\n",
        "\t\tintersection_area = torch.min(box1[..., 0], box2[..., 0]) * torch.min(box1[..., 1], box2[..., 1])\n",
        "\n",
        "\t\t# Calculate union area\n",
        "\t\tbox1_area = box1[..., 0] * box1[..., 1]\n",
        "\t\tbox2_area = box2[..., 0] * box2[..., 1]\n",
        "\t\tunion_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "\t\t# Calculate IoU score\n",
        "\t\tiou_score = intersection_area / union_area\n",
        "\n",
        "\t\t# Return IoU score\n",
        "\t\treturn iou_score\n"
      ],
      "metadata": {
        "id": "6KJHFXVw3xBR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-maximum suppression function to remove overlapping bounding boxes\n",
        "def nms(bboxes, iou_threshold, threshold):\n",
        "\t# Filter out bounding boxes with confidence below the threshold.\n",
        "\tbboxes = [box for box in bboxes if box[1] > threshold]\n",
        "\n",
        "\t# Sort the bounding boxes by confidence in descending order.\n",
        "\tbboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\t# Initialize the list of bounding boxes after non-maximum suppression.\n",
        "\tbboxes_nms = []\n",
        "\n",
        "\twhile bboxes:\n",
        "\t\t# Get the first bounding box.\n",
        "\t\tfirst_box = bboxes.pop(0)\n",
        "\n",
        "\t\t# Iterate over the remaining bounding boxes.\n",
        "\t\tfor box in bboxes:\n",
        "\t\t# If the bounding boxes do not overlap or if the first bounding box has\n",
        "\t\t# a higher confidence, then add the second bounding box to the list of\n",
        "\t\t# bounding boxes after non-maximum suppression.\n",
        "\t\t\tif box[0] != first_box[0] or iou(\n",
        "\t\t\t\ttorch.tensor(first_box[2:]),\n",
        "\t\t\t\ttorch.tensor(box[2:]),\n",
        "\t\t\t) < iou_threshold:\n",
        "\t\t\t\t# Check if box is not in bboxes_nms\n",
        "\t\t\t\tif box not in bboxes_nms:\n",
        "\t\t\t\t\t# Add box to bboxes_nms\n",
        "\t\t\t\t\tbboxes_nms.append(box)\n",
        "\n",
        "\t# Return bounding boxes after non-maximum suppression.\n",
        "\treturn bboxes_nms\n"
      ],
      "metadata": {
        "id": "o07FHp2V33v3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert cells to bounding boxes\n",
        "def convert_cells_to_bboxes(predictions, anchors, s, is_predictions=True):\n",
        "\t# Batch size used on predictions\n",
        "\tbatch_size = predictions.shape[0]\n",
        "\t# Number of anchors\n",
        "\tnum_anchors = len(anchors)\n",
        "\t# List of all the predictions\n",
        "\tbox_predictions = predictions[..., 1:5]\n",
        "\n",
        "\t# If the input is predictions then we will pass the x and y coordinate\n",
        "\t# through sigmoid function and width and height to exponent function and\n",
        "\t# calculate the score and best class.\n",
        "\tif is_predictions:\n",
        "\t\tanchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "\t\tbox_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "\t\tbox_predictions[..., 2:] = torch.exp(\n",
        "\t\t\tbox_predictions[..., 2:]) * anchors\n",
        "\t\tscores = torch.sigmoid(predictions[..., 0:1])\n",
        "\t\tbest_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "\n",
        "\t# Else we will just calculate scores and best class.\n",
        "\telse:\n",
        "\t\tscores = predictions[..., 0:1]\n",
        "\t\tbest_class = predictions[..., 5:6]\n",
        "\n",
        "\t# Calculate cell indices\n",
        "\tcell_indices = (\n",
        "\t\ttorch.arange(s)\n",
        "\t\t.repeat(predictions.shape[0], 3, s, 1)\n",
        "\t\t.unsqueeze(-1)\n",
        "\t\t.to(predictions.device)\n",
        "\t)\n",
        "\n",
        "\t# Calculate x, y, width and height with proper scaling\n",
        "\tx = 1 / s * (box_predictions[..., 0:1] + cell_indices)\n",
        "\ty = 1 / s * (box_predictions[..., 1:2] +\n",
        "\t\t\t\tcell_indices.permute(0, 1, 3, 2, 4))\n",
        "\twidth_height = 1 / s * box_predictions[..., 2:4]\n",
        "\n",
        "\t# Concatinating the values and reshaping them in\n",
        "\t# (BATCH_SIZE, num_anchors * S * S, 6) shape\n",
        "\tconverted_bboxes = torch.cat(\n",
        "\t\t(best_class, scores, x, y, width_height), dim=-1\n",
        "\t).reshape(batch_size, num_anchors * s * s, 6)\n",
        "\n",
        "\t# Returning the reshaped and converted bounding box list\n",
        "\treturn converted_bboxes.tolist()"
      ],
      "metadata": {
        "id": "39wdIiWh3-i2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot images with bounding boxes and class labels\n",
        "def plot_image(image, boxes):\n",
        "\t# Getting the color map from matplotlib\n",
        "\tcolour_map = plt.get_cmap(\"tab20b\")\n",
        "\t# Getting 20 different colors from the color map for 20 different classes\n",
        "\tcolors = [colour_map(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "\n",
        "\t# Reading the image with OpenCV\n",
        "\timg = np.array(image)\n",
        "\t# Getting the height and width of the image\n",
        "\th, w, _ = img.shape\n",
        "\n",
        "\t# Create figure and axes\n",
        "\tfig, ax = plt.subplots(1)\n",
        "\n",
        "\t# Add image to plot\n",
        "\tax.imshow(img)\n",
        "\n",
        "\t# Plotting the bounding boxes and labels over the image\n",
        "\tfor box in boxes:\n",
        "\t\t# Get the class from the box\n",
        "\t\tclass_pred = box[0]\n",
        "\t\t# Get the center x and y coordinates\n",
        "\t\tbox = box[2:]\n",
        "\t\t# Get the upper left corner coordinates\n",
        "\t\tupper_left_x = box[0] - box[2] / 2\n",
        "\t\tupper_left_y = box[1] - box[3] / 2\n",
        "\n",
        "\t\t# Create a Rectangle patch with the bounding box\n",
        "\t\trect = patches.Rectangle(\n",
        "\t\t\t(upper_left_x * w, upper_left_y * h),\n",
        "\t\t\tbox[2] * w,\n",
        "\t\t\tbox[3] * h,\n",
        "\t\t\tlinewidth=2,\n",
        "\t\t\tedgecolor=colors[int(class_pred)],\n",
        "\t\t\tfacecolor=\"none\",\n",
        "\t\t)\n",
        "\n",
        "\t\t# Add the patch to the Axes\n",
        "\t\tax.add_patch(rect)\n",
        "\n",
        "\t\t# Add class name to the patch\n",
        "\t\tplt.text(\n",
        "\t\t\tupper_left_x * w,\n",
        "\t\t\tupper_left_y * h,\n",
        "\t\t\ts=class_labels[int(class_pred)],\n",
        "\t\t\tcolor=\"white\",\n",
        "\t\t\tverticalalignment=\"top\",\n",
        "\t\t\tbbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "\t\t)\n",
        "\n",
        "\t# Display the plot\n",
        "\tplt.show()\n"
      ],
      "metadata": {
        "id": "zbG6GCg94AuR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save checkpoint\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "\tprint(\"==> Saving checkpoint\")\n",
        "\tcheckpoint = {\n",
        "\t\t\"state_dict\": model.state_dict(),\n",
        "\t\t\"optimizer\": optimizer.state_dict(),\n",
        "\t}\n",
        "\ttorch.save(checkpoint, filename)\n"
      ],
      "metadata": {
        "id": "QWmauuHC4JxV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load checkpoint\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "\tprint(\"==> Loading checkpoint\")\n",
        "\tcheckpoint = torch.load(checkpoint_file, map_location=device)\n",
        "\tmodel.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\toptimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "\tfor param_group in optimizer.param_groups:\n",
        "\t\tparam_group[\"lr\"] = lr\n"
      ],
      "metadata": {
        "id": "wKzWjbSG4MT2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load and save model variable\n",
        "load_model = False\n",
        "save_model = True\n",
        "\n",
        "# model checkpoint file name\n",
        "checkpoint_file = \"checkpoint.pth.tar\"\n",
        "\n",
        "# Anchor boxes for each feature map scaled between 0 and 1\n",
        "# 3 feature maps at 3 different scales based on YOLOv3 paper\n",
        "ANCHORS = [\n",
        "\t[(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "\t[(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "\t[(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "]\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Learning rate for training\n",
        "leanring_rate = 1e-5\n",
        "\n",
        "# Number of epochs for training\n",
        "epochs = 20\n",
        "\n",
        "# Image size\n",
        "image_size = 416\n",
        "\n",
        "# Grid cell sizes\n",
        "s = [image_size // 32, image_size // 16, image_size // 8]\n",
        "\n",
        "# Class labels\n",
        "class_labels = [\n",
        "\t\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "\t\"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "\t\"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "Dg-O72qI4RJK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset class to load the images and labels from the folder\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\tdef __init__(\n",
        "\t\tself, csv_file, image_dir, label_dir, anchors,\n",
        "\t\timage_size=416, grid_sizes=[13, 26, 52],\n",
        "\t\tnum_classes=20, transform=None\n",
        "\t):\n",
        "\t\t# Read the csv file with image names and labels\n",
        "\t\tself.label_list = pd.read_csv(csv_file)\n",
        "\t\t# Image and label directories\n",
        "\t\tself.image_dir = image_dir\n",
        "\t\tself.label_dir = label_dir\n",
        "\t\t# Image size\n",
        "\t\tself.image_size = image_size\n",
        "\t\t# Transformations\n",
        "\t\tself.transform = transform\n",
        "\t\t# Grid sizes for each scale\n",
        "\t\tself.grid_sizes = grid_sizes\n",
        "\t\t# Anchor boxes\n",
        "\t\tself.anchors = torch.tensor(\n",
        "\t\t\tanchors[0] + anchors[1] + anchors[2])\n",
        "\t\t# Number of anchor boxes\n",
        "\t\tself.num_anchors = self.anchors.shape[0]\n",
        "\t\t# Number of anchor boxes per scale\n",
        "\t\tself.num_anchors_per_scale = self.num_anchors // 3\n",
        "\t\t# Number of classes\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\t# Ignore IoU threshold\n",
        "\t\tself.ignore_iou_thresh = 0.5\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.label_list)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\t# Getting the label path\n",
        "\t\tlabel_path = os.path.join(self.label_dir, self.label_list.iloc[idx, 1])\n",
        "\t\t# We are applying roll to move class label to the last column\n",
        "\t\t# 5 columns: x, y, width, height, class_label\n",
        "\t\tbboxes = np.roll(np.loadtxt(fname=label_path,\n",
        "\t\t\t\t\t\tdelimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "\n",
        "\t\t# Getting the image path\n",
        "\t\timg_path = os.path.join(self.image_dir, self.label_list.iloc[idx, 0])\n",
        "\t\timage = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "\t\t# Albumentations augmentations\n",
        "\t\tif self.transform:\n",
        "\t\t\taugs = self.transform(image=image, bboxes=bboxes)\n",
        "\t\t\timage = augs[\"image\"]\n",
        "\t\t\tbboxes = augs[\"bboxes\"]\n",
        "\n",
        "\t\t# Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
        "\t\t# target : [probabilities, x, y, width, height, class_label]\n",
        "\t\ttargets = [torch.zeros((self.num_anchors_per_scale, s, s, 6))\n",
        "\t\t\t\tfor s in self.grid_sizes]\n",
        "\n",
        "\t\t# Identify anchor box and cell for each bounding box\n",
        "\t\tfor box in bboxes:\n",
        "\t\t\t# Calculate iou of bounding box with anchor boxes\n",
        "\t\t\tiou_anchors = iou(torch.tensor(box[2:4]),\n",
        "\t\t\t\t\t\t\tself.anchors,\n",
        "\t\t\t\t\t\t\tis_pred=False)\n",
        "\t\t\t# Selecting the best anchor box\n",
        "\t\t\tanchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "\t\t\tx, y, width, height, class_label = box\n",
        "\n",
        "\t\t\t# At each scale, assigning the bounding box to the\n",
        "\t\t\t# best matching anchor box\n",
        "\t\t\thas_anchor = [False] * 3\n",
        "\t\t\tfor anchor_idx in anchor_indices:\n",
        "\t\t\t\tscale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "\t\t\t\tanchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "\n",
        "\t\t\t\t# Identifying the grid size for the scale\n",
        "\t\t\t\ts = self.grid_sizes[scale_idx]\n",
        "\n",
        "\t\t\t\t# Identifying the cell to which the bounding box belongs\n",
        "\t\t\t\ti, j = int(s * y), int(s * x)\n",
        "\t\t\t\tanchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "\n",
        "\t\t\t\t# Check if the anchor box is already assigned\n",
        "\t\t\t\tif not anchor_taken and not has_anchor[scale_idx]:\n",
        "\n",
        "\t\t\t\t\t# Set the probability to 1\n",
        "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "\n",
        "\t\t\t\t\t# Calculating the center of the bounding box relative\n",
        "\t\t\t\t\t# to the cell\n",
        "\t\t\t\t\tx_cell, y_cell = s * x - j, s * y - i\n",
        "\n",
        "\t\t\t\t\t# Calculating the width and height of the bounding box\n",
        "\t\t\t\t\t# relative to the cell\n",
        "\t\t\t\t\twidth_cell, height_cell = (width * s, height * s)\n",
        "\n",
        "\t\t\t\t\t# Idnetify the box coordinates\n",
        "\t\t\t\t\tbox_coordinates = torch.tensor(\n",
        "\t\t\t\t\t\t\t\t\t\t[x_cell, y_cell, width_cell,\n",
        "\t\t\t\t\t\t\t\t\t\theight_cell]\n",
        "\t\t\t\t\t\t\t\t\t)\n",
        "\n",
        "\t\t\t\t\t# Assigning the box coordinates to the target\n",
        "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "\n",
        "\t\t\t\t\t# Assigning the class label to the target\n",
        "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "\n",
        "\t\t\t\t\t# Set the anchor box as assigned for the scale\n",
        "\t\t\t\t\thas_anchor[scale_idx] = True\n",
        "\n",
        "\t\t\t\t# If the anchor box is already assigned, check if the\n",
        "\t\t\t\t# IoU is greater than the threshold\n",
        "\t\t\t\telif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "\t\t\t\t\t# Set the probability to -1 to ignore the anchor box\n",
        "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
        "\n",
        "\t\t# Return the image and the target\n",
        "\t\treturn image, tuple(targets)\n"
      ],
      "metadata": {
        "id": "BJaM2VZG4T2S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3-F5yFqi5fN3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}