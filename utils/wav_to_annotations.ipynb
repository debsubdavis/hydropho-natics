{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this code is to tie vggish points back to their annotations. We do this in 5 major steps. \n",
    "\n",
    "1. We pull the most recent annotation file for each image. This was created in a previous script called \"unique_images_annotations.ipynb\". Run it now to generate the csv by the same name which is required for this script.\n",
    "2. Opening the most recent JSON for each image we get the annotations and their coordinates.\n",
    "3. Each annotated image has an associated file number. These file numbers directly relate to the windows covered e.g., file-0 covers from 0 to 1 minute of time in original audio, file-1 covers from minute 1 to minute 2 of original audio, etc. We will use these file numbers to get the spectrogram start time in seconds.\n",
    "4. To find where each noise specifically happened we pull the coordinates of the annotations. By finding how far the annotation started and ended in the spectrogram and knowing that each spectrogram accounts for a minute, we can get the start and end time of a sound in seconds.\n",
    "5. Finally, we need to match these annotation start and stop times back to the VGGish points. We know that each VGGish point accounts for 0.96 seconds of raw audio time, so dividing the start and stop sound times by 0.96 will give us which slice each belongs to.\n",
    "\n",
    "Step 1: We start by loading basic Python libraries and files for use (the unique image annotations file noted above, as well as the mapping file which ties wav filenames back to the shortened labels for VGGish's output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#File imports\n",
    "uniq_annotation_df = pd.read_csv('unique_images_annotations.csv')\n",
    "mapping_file_df = pd.read_csv('mapping_filenames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: We begin by opening each JSON in the uniq_annotation_df and pulling out the image, sound and annotation points info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a list of all the unique JSON files\n",
    "json_file_list = list(uniq_annotation_df['json_file_path'])\n",
    "\n",
    "#Creating a df to save data from the JSON in\n",
    "annotation_df = pd.DataFrame(columns=['sound','points','image_name', 'json_file_path'])\n",
    "\n",
    "#Iterate through json files on external hard drive to get annotated image info\n",
    "for file in json_file_list:\n",
    "    #Loading the JSON data & turning into dict\n",
    "    annotated_file = open(file)\n",
    "    annotated_dict = json.load(annotated_file)\n",
    "    #Pulling out the labels, points, and image path\n",
    "    image_name = annotated_dict['imagePath']\n",
    "    if len(annotated_dict['shapes']) > 0:\n",
    "        for shape in annotated_dict['shapes']:\n",
    "            sound = shape['label']\n",
    "            points = shape['points']\n",
    "            annotation_df.loc[len(annotation_df.index)] = [sound, points, image_name, file]\n",
    "    else:\n",
    "        sound = None\n",
    "        points = None\n",
    "        annotation_df.loc[len(annotation_df.index)] = [sound, points, image_name, file]\n",
    "    \n",
    "#Validating we got all the json files from the original unique annotations dataframe in the new annotation df\n",
    "if len(uniq_annotation_df) != len(annotation_df['json_file_path'].unique()):\n",
    "    print(\"ERROR - not all JSON contained in the annotation_df\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Validating that images paired with json from unique annotations file are the same as the images referenced in the JSON files themselves\n",
    "comb_df = pd.merge(left = uniq_annotation_df, right = annotation_df, how = 'left', left_on='json_file_path', right_on='json_file_path')\n",
    "comb_df['image_file_name_tester'] = comb_df['image_file_name']+'.png'\n",
    "comb_df['name_compare'] = np.where((comb_df['image_file_name_tester'] == comb_df['image_name']), 0, 1)\n",
    "if comb_df['name_compare'].sum() != 0:\n",
    "    print(\"ERROR - The following json files contain different image filenames than those they were aligned to in the unique_image_annotations code\")\n",
    "    print(comb_df.loc[comb_df['name_compare']!=0])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Dropping the JSON file path column as it will no longer be used\n",
    "annotation_df = annotation_df.drop(['json_file_path'], axis = 1)\n",
    "\n",
    "#Removing rows which images which no sounds since they aren't useful for what we want to do\n",
    "annotation_df = annotation_df[annotation_df['sound'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Each annotated image has an associated file number. These file numbers directly relate to the windows covered e.g., file-0 covers from 0 to 1 minute of time in original audio, file-1 covers from minute 1 to minute 2 of original audio, etc. We will use these file numbers to get the spectrogram start time in seconds. We add a column which contains the file number only and multiply it by 60 to get the number of seconds that have passed in the audio before the spectrogram starts. The new column is called \"spectrogram_start_sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting just the image file number and multiplying by 60 seconds\n",
    "annotation_df['spectrogram_start_min'] = annotation_df['image_name'].str[21:-4]\n",
    "annotation_df['spectrogram_start_sec'] = pd.to_numeric(annotation_df['spectrogram_start_min'])*60\n",
    "annotation_df = annotation_df.drop('spectrogram_start_min', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: To find where each noise specifically happened we pull the coordinates of the annotations. By finding how far the annotation started and ended in the spectrogram and knowing that each spectrogram accounts for a minute, we can get the start and end time of a sound in seconds.\n",
    "\n",
    "We start by getting x1 and x2 from the coordinates of the bounding boxes. The min is the start time in pixels, and the max is the stop time in pixels. We then subtract the distance from the edge of the image to the spectrogram to ensure we aren't counting white space in the image which isn't relevent to the audio. Then, by finding the percent of the way through the spectrogram the annotation starts we can multiply by 60 (60 secs per spectrogram) to understand where the sound started. Adding the spectrogram_start_sec will tell you the time in the audio file the sound started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting individual points from points column\n",
    "annotation_df['point1'] = annotation_df['points'].str[0]\n",
    "annotation_df['point2'] = annotation_df['points'].str[1]\n",
    "\n",
    "#Getting x1 and x2 from point1 and point2\n",
    "annotation_df['x1'] = annotation_df['point1'].str[0]\n",
    "annotation_df['x2'] = annotation_df['point2'].str[0]\n",
    "\n",
    "#Finding the start vs. the stop time\n",
    "annotation_df['annotation_start'] = annotation_df[[\"x1\", \"x2\"]].min(axis=1)\n",
    "annotation_df['annotation_stop'] = annotation_df[[\"x1\", \"x2\"]].max(axis=1)\n",
    "\n",
    "#Subtract the left edge of the spectrogram in pixels (309.532) from the annotation start and stops\n",
    "#to ensure we don't count whitespace in the image which has no relevance to the audio in the wav files\n",
    "annotation_df['annotation_start'] = annotation_df['annotation_start'] - 309.532\n",
    "annotation_df['annotation_stop'] = annotation_df['annotation_stop'] - 309.532\n",
    "\n",
    "#Divide the start & stop times by the total pixels in the spectrogram (2173.266 - 309.532)\n",
    "#& multiply by 60 to get time in seconds\n",
    "annotation_df['annotation_start_sec'] = annotation_df['annotation_start'] * 60 / (2173.266- 309.532)\n",
    "annotation_df['annotation_stop_sec'] = annotation_df['annotation_stop'] * 60 / (2173.266- 309.532)\n",
    "\n",
    "#Adding the spectrogram_start_time_secs to annotation_start_sec to get the final annotation start/stop time\n",
    "annotation_df['time_in_wav_start_sec'] = annotation_df['annotation_start_sec'] + annotation_df['spectrogram_start_sec']\n",
    "annotation_df['time_in_wav_stop_sec'] = annotation_df['annotation_stop_sec'] + annotation_df['spectrogram_start_sec']\n",
    "\n",
    "#Removing all the calculation columns\n",
    "annotation_df = annotation_df.drop(['points','spectrogram_start_sec','point1','point2','x1','x2','annotation_start','annotation_stop','annotation_start_sec','annotation_stop_sec'], axis=1)\n",
    "#annotation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Finally, we need to match these annotation start and stop times back to the VGGish points. We know that each VGGish point accounts for 0.96 seconds of raw audio time, so dividing the start and stop sound times by 0.96 will give us which slice each belongs to.\n",
    "\n",
    "We create an intermediate column with the range of slices that cover the sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a col with the start image slice and the end image slice by dividing the time by 0.96 for the seconds in the spectrogram\n",
    "annotation_df['start_slice'] = annotation_df['time_in_wav_start_sec'] / 0.96\n",
    "annotation_df['stop_slice'] = annotation_df['time_in_wav_stop_sec'] / 0.96\n",
    "\n",
    "#Taking the floor of the start slice and the ceiling of the stop slice to ensure we get all the sound\n",
    "#in our slices\n",
    "annotation_df['start_slice'] = annotation_df['start_slice'].apply(np.floor)\n",
    "annotation_df['stop_slice'] = annotation_df['stop_slice'].apply(np.ceil)\n",
    "\n",
    "#Creating column which is the list of the slices between start_slice and stop_slice\n",
    "start_slices = list(annotation_df['start_slice'])\n",
    "stop_slices = list(annotation_df['stop_slice'])\n",
    "slice_range = []\n",
    "for i in range(len(start_slices)):\n",
    "    range_list = list(range(int(start_slices[i]), int(stop_slices[i])+1))\n",
    "    slice_range.append(range_list)\n",
    "\n",
    "#Adding a col which is the range of slice numbers per sound\n",
    "annotation_df['slice_numbers'] = slice_range\n",
    "\n",
    "#Dropping the start and stop slice cols\n",
    "annotation_df = annotation_df.drop(['start_slice','stop_slice'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 (continued): Now we need to map the wav files in the mapping_filenames.csv back to the images. We do this using the YYMMDDTHHMMS in the image_name to match back to the wav filename's first 12 digits which are also YYMMDD-HHMMS, and joining to get the file lookup number. \n",
    "\n",
    "We don't match on the final 's' value in either filename because there was a batch processing issue with the original files which misrecorded '2' as '4' and will create non-matches between files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column with the image name in a format that is matchable back to the wav file name in th e\n",
    "#mapping_filenames.csv file\n",
    "annotation_df['image_name_wav_format'] = (annotation_df['image_name'].str[2:8] + '-' + annotation_df['image_name'].str[9:14])\n",
    "\n",
    "#Getting the matching characters from the mapping_filenames wav names\n",
    "mapping_file_df['wav_name_match'] = mapping_file_df['wav_filename'].str[:12]\n",
    "\n",
    "#Matching the annotation_df and mapping_filenames df together\n",
    "wav_annotation_df = pd.merge(left = annotation_df, right = mapping_file_df, how = 'left',\n",
    "                             left_on='image_name_wav_format', right_on='wav_name_match')\n",
    "\n",
    "#Checking match\n",
    "if len(wav_annotation_df) == len(annotation_df):\n",
    "    pass\n",
    "else:\n",
    "    print(\"ERROR - unsuccessful merge, rows differ between annotation_df and wav_annotation_df. Check code.\")\n",
    "\n",
    "#Removing extra columns used to merge datasets\n",
    "wav_annotation_df = wav_annotation_df.drop(['image_name_wav_format','wav_name_match'], axis = 1)\n",
    "\n",
    "#Saving the full output for manual listening use\n",
    "wav_annotation_df.to_csv('human_wav_to_annotation_ref.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 (continued): Now we pull apart the 'slice_numbers' column to create a row per slice number. We create a final column which shows the mapped_filename-slice_number which is the unique label on each vggish point. We save this focused data frame for easy reference in model output clustering scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of sounds and the list of slice_numbers\n",
    "sounds_list = list(wav_annotation_df['sound'])\n",
    "image_name_list = list(wav_annotation_df['image_name'])\n",
    "mapped_filename_list = list(wav_annotation_df['mapped_filename'])\n",
    "slice_numbers_list = list(wav_annotation_df['slice_numbers'])\n",
    "\n",
    "#Creating final table w/ sound, image_name, vggish_point\n",
    "wav_to_annotation_df = pd.DataFrame(columns= ['sound','image_name','vggish_point'])\n",
    "\n",
    "#Iterate through each slice numbers array, pull it apart, save each element + the mapped filename and sound to a new df\n",
    "for i in range(len(slice_numbers_list)):\n",
    "    sound = sounds_list[i]\n",
    "    image_name = image_name_list[i]\n",
    "    mapped_filename = mapped_filename_list[i]\n",
    "    slices = slice_numbers_list[i]\n",
    "    for slice in slices:\n",
    "        vggish_point = str(mapped_filename) + '-' + str(slice)\n",
    "        wav_to_annotation_df.loc[len(wav_to_annotation_df)] = [sound, image_name, vggish_point]\n",
    "\n",
    "#Checking that all slices are in the final wav_to_annotation_df\n",
    "slice_count = 0\n",
    "for slice in slice_numbers_list:\n",
    "    slice_count = slice_count + len(slice)\n",
    "if slice_count != len(wav_to_annotation_df):\n",
    "    print(\"ERROR - file wrong length. Check code to ensure all slices are included.\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Saving the file\n",
    "wav_to_annotation_df.to_csv('wav_to_annotation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggish_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
