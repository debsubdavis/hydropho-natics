{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this code is to tie annotations to their time of occurrance in WAV files and map to points output by VGGish. We do this in 6 major steps.\n",
    "\n",
    "*If the file human_readable_annotation_timings.csv already exists, skip to Part II of the code*\n",
    "\n",
    "### Part I - creates human_readable_annotation_timings.csv\n",
    "1. We pull the most recent annotation file for each image. This was created in a previous script called \"unique_images_annotations.ipynb\". Run it now if you haven't already to generate the unique_images_annotations.csv which is required for this script.\n",
    "2. Open the most recent JSON for each image to get the annotations and their coordinates.\n",
    "3. Each annotated image has an associated file number. These file numbers directly relate to the windows covered e.g., file-0 covers from 0 to 1 minute of time in original audio, file-1 covers from minute 1 to minute 2 of original audio, etc. We will use these file numbers to get the spectrogram start time in seconds.\n",
    "4. To find where each noise specifically happened we pull the coordinates of the annotations. By finding how far the annotation started and ended in the spectrogram and knowing that each spectrogram accounts for a minute, we can get the start and end time of a sound in seconds. This output is saved as human_readable_annotation_timings.csv\n",
    "\n",
    "### Part II - creates vggish_X_sec_comb_labels.csv\n",
    "1. We need to match these annotation start and stop times back to the VGGish points. We know that each VGGish point accounts for X seconds of raw audio time (denoted by the variable exmaple_duration), so dividing the start and stop sound times by X will give us which example each belongs to.\n",
    "2. Becaue we are using each annotation as the label of the vggish_point in later clustering algorithms we need to ensure each point has a single label. This requires combining labels for vggish_points with multiple annotations. This output is saved as wav_to_annotation_combined_labels.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "\n",
    "Step 1: We start by loading basic Python libraries and files for use (unique_images_annotations.csv, as well as the mapping file which ties wav filenames back to the shortened labels for VGGish's output developed by Saumya). We also set a constant value \"example_duration\" which will be used in step 5's calculation of examples - this value can changed based on model imputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#File imports\n",
    "uniq_annotation_df = pd.read_csv('unique_images_annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: We begin by opening each JSON in the uniq_annotation_df and pulling out the image, sound and annotation points info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a list of all the unique JSON files\n",
    "json_file_list = list(uniq_annotation_df['json_file_path'])\n",
    "\n",
    "#Creating a df to save data from the JSON in\n",
    "annotation_df = pd.DataFrame(columns=['sound','points','image_name', 'json_file_path'])\n",
    "\n",
    "#Iterate through json files on external hard drive to get annotated image info\n",
    "for file in json_file_list:\n",
    "    #Loading the JSON data & turning into dict\n",
    "    annotated_file = open(file)\n",
    "    annotated_dict = json.load(annotated_file)\n",
    "    #Pulling out the labels, points, and image path\n",
    "    image_name = annotated_dict['imagePath']\n",
    "    if len(annotated_dict['shapes']) > 0:\n",
    "        for shape in annotated_dict['shapes']:\n",
    "            sound = shape['label']\n",
    "            points = shape['points']\n",
    "            annotation_df.loc[len(annotation_df.index)] = [sound, points, image_name, file]\n",
    "    else:\n",
    "        sound = None\n",
    "        points = None\n",
    "        annotation_df.loc[len(annotation_df.index)] = [sound, points, image_name, file]\n",
    "    \n",
    "#Validating we got all the json files from the original unique annotations dataframe in the new annotation df\n",
    "if len(uniq_annotation_df) != len(annotation_df['json_file_path'].unique()):\n",
    "    print(\"ERROR - not all JSON contained in the annotation_df\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Validating that images paired with json from unique annotations file are the same as the images referenced in the JSON files themselves\n",
    "comb_df = pd.merge(left = uniq_annotation_df, right = annotation_df, how = 'left', left_on='json_file_path', right_on='json_file_path')\n",
    "comb_df['image_file_name_tester'] = comb_df['image_file_name']+'.png'\n",
    "comb_df['name_compare'] = np.where((comb_df['image_file_name_tester'] == comb_df['image_name']), 0, 1)\n",
    "if comb_df['name_compare'].sum() != 0:\n",
    "    print(\"ERROR - The following json files contain different image filenames than those they were aligned to in the unique_image_annotations code\")\n",
    "    print(comb_df.loc[comb_df['name_compare']!=0])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Dropping the JSON file path column as it will no longer be used\n",
    "annotation_df = annotation_df.drop(['json_file_path'], axis = 1)\n",
    "\n",
    "#Removing rows with images with no sounds since they aren't useful for what we want to do\n",
    "annotation_df = annotation_df[annotation_df['sound'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Each annotated image has an associated file number. These file numbers directly relate to the windows covered e.g., file-0 covers from 0 to 1 minute of time in original audio, file-1 covers from minute 1 to minute 2 of original audio, etc. We will use these file numbers to get the spectrogram start time in seconds. We add a column which contains the file number only and multiply it by 60 to get the number of seconds that have passed in the audio before the spectrogram starts. The new column is called \"spectrogram_start_sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting just the image file number and multiplying by 60 seconds\n",
    "annotation_df['spectrogram_start_min'] = annotation_df['image_name'].str[21:-4]\n",
    "annotation_df['spectrogram_start_sec'] = pd.to_numeric(annotation_df['spectrogram_start_min'])*60\n",
    "annotation_df = annotation_df.drop('spectrogram_start_min', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: To find where each noise specifically happened we pull the coordinates of the annotations. By finding how far the annotation started and ended in the spectrogram and knowing that each spectrogram accounts for a minute, we can get the start and end time of a sound in seconds.\n",
    "\n",
    "We start by getting x1 and x2 from the coordinates of the bounding boxes. The min is the start time in pixels, and the max is the stop time in pixels. We then subtract the distance from the edge of the image to the spectrogram to ensure we aren't counting white space in the image which isn't relevent to the audio. Then, by finding the percent of the way through the spectrogram the annotation starts we can multiply by 60 (60 secs per spectrogram) to understand where the sound started. Adding the spectrogram_start_sec will tell you the time in the audio file the sound started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting individual points from points column\n",
    "annotation_df['point1'] = annotation_df['points'].str[0]\n",
    "annotation_df['point2'] = annotation_df['points'].str[1]\n",
    "\n",
    "#Getting x1 and x2 from point1 and point2\n",
    "annotation_df['x1'] = annotation_df['point1'].str[0]\n",
    "annotation_df['x2'] = annotation_df['point2'].str[0]\n",
    "\n",
    "#Finding the start vs. the stop time\n",
    "annotation_df['annotation_start'] = round(annotation_df[[\"x1\", \"x2\"]].min(axis=1), 3)\n",
    "annotation_df['annotation_stop'] = round(annotation_df[[\"x1\", \"x2\"]].max(axis=1), 3)\n",
    "\n",
    "#Subtract the avg. left edge of the spectrogram in pixels (309.532) from the annotation start and stops\n",
    "#to ensure we don't count whitespace in the image which has no relevance to the audio in the wav files\n",
    "annotation_df['annotation_start_shifted'] = annotation_df['annotation_start'] - 309.532\n",
    "annotation_df['annotation_stop_shifted'] = annotation_df['annotation_stop'] - 309.532\n",
    "\n",
    "#Divide the start & stop times by the total avg pixels in the spectrogram (1863.734)\n",
    "#& multiply by 60 to get time in seconds\n",
    "annotation_df['annotation_start_sec'] = annotation_df['annotation_start_shifted'] * 60.000 / 1863.734\n",
    "annotation_df['annotation_stop_sec'] = annotation_df['annotation_stop_shifted'] * 60.000 / 1863.734\n",
    "\n",
    "#Adding the spectrogram_start_time_secs to annotation_start_sec to get the final annotation start/stop time\n",
    "annotation_df['time_in_wav_start_sec'] = annotation_df['annotation_start_sec'] + annotation_df['spectrogram_start_sec']\n",
    "annotation_df['time_in_wav_stop_sec'] = annotation_df['annotation_stop_sec'] + annotation_df['spectrogram_start_sec']\n",
    "\n",
    "#Removing all the calculation columns\n",
    "annotation_df = annotation_df.drop(['points','spectrogram_start_sec','point1','point2','x1','x2','annotation_start','annotation_stop','annotation_start_sec','annotation_stop_sec', 'annotation_start_shifted', 'annotation_stop_shifted'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the current annotation data frame as \"human_readable_annotation_timings.csv\". The format is useful for humans to double check the location of annotations in wav files. In Part II of the code below we will change the format to be more machine-friendly and serve as an input in future clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the annotation timings in a human-readable format\n",
    "annotation_df.to_csv('human_readable_annotation_timings.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "In Part II of the code we alter \"human_readable_annotation_timings.csv\" to be machine-friendly and serve as data label inputs in future clustering models. We start by importing files and libraries necessary for Part II of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#File imports\n",
    "mapping_file_df = pd.read_csv('mapping_filenames.csv')\n",
    "annotation_df = pd.read_csv('human_readable_annotation_timings.csv')\n",
    "\n",
    "#Setting duration of VGGish examples\n",
    "example_duration = 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: We need to match annotation start and stop times in human_readable_annotations back to the VGGish points. We know that each VGGish point accounts for X seconds of raw audio time (denoted by the variable example_duration above), so dividing the start and stop sound times by X will give us which example each belongs to. We create an intermediate column with the range of examples that cover the sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - example below min example present in data, verify close enough\n",
      "     sound                   image_name  time_in_wav_start_sec  \\\n",
      "1833  fish  20181227T100004-File-20.png            1199.990439   \n",
      "\n",
      "      time_in_wav_stop_sec  start_example_float  stop_example_float  \\\n",
      "1833           1209.175032           1249.99004         1259.557326   \n",
      "\n",
      "      start_example  stop_example  \\\n",
      "1833         1249.0        1259.0   \n",
      "\n",
      "                                        example_numbers image_number  \\\n",
      "1833  [1249, 1250, 1251, 1252, 1253, 1254, 1255, 125...           20   \n",
      "\n",
      "      min_example  max_example  min_example_floor  max_example_floor  \\\n",
      "1833       1250.0       1312.5             1250.0             1312.0   \n",
      "\n",
      "      min_bounds_exceeded  max_bounds_exceeded  \n",
      "1833                    1                    0  \n"
     ]
    }
   ],
   "source": [
    "#Creating a col with the starting example and ending example by dividing the time by example_duration\n",
    "#for the seconds in the spectrogram\n",
    "annotation_df['start_example_float'] = annotation_df['time_in_wav_start_sec'] / example_duration\n",
    "annotation_df['stop_example_float'] = annotation_df['time_in_wav_stop_sec'] / example_duration\n",
    "\n",
    "#Taking the floor of the start example and stop example to ensure we get all the sound\n",
    "#in our examples (rounding the start and stop times down to the nearest example)\n",
    "annotation_df['start_example'] = annotation_df['start_example_float'].apply(np.floor)\n",
    "annotation_df['stop_example'] = annotation_df['stop_example_float'].apply(np.floor)\n",
    "\n",
    "#Creating column which is the list of the examples between start_example and stop_example\n",
    "start_examples = list(annotation_df['start_example'])\n",
    "stop_examples = list(annotation_df['stop_example'])\n",
    "example_range = []\n",
    "for i in range(len(start_examples)):\n",
    "    range_list = list(range(int(start_examples[i]), int(stop_examples[i])+1))\n",
    "    example_range.append(range_list)\n",
    "\n",
    "#Adding a col which is the range of example numbers per sound\n",
    "annotation_df['example_numbers'] = example_range\n",
    "\n",
    "#Verifying that no start/stop examples occurr outside the expected timeframe based on the image name\n",
    "test_annotation_df = annotation_df.copy()\n",
    "test_annotation_df['image_number'] = test_annotation_df['image_name'].str[21:-4]\n",
    "test_annotation_df['min_example'] = (pd.to_numeric(test_annotation_df['image_number'])*60/0.96)\n",
    "test_annotation_df['max_example'] = ((pd.to_numeric(test_annotation_df['image_number'])+1)*60/0.96)\n",
    "test_annotation_df['min_example_floor'] = test_annotation_df['min_example'].apply(np.floor)\n",
    "test_annotation_df['max_example_floor'] = test_annotation_df['max_example'].apply(np.floor)\n",
    "test_annotation_df['min_bounds_exceeded'] = np.where((test_annotation_df['start_example'] < test_annotation_df['min_example_floor']), 1, 0)\n",
    "test_annotation_df['max_bounds_exceeded'] = np.where((test_annotation_df['stop_example'] > test_annotation_df['max_example_floor']), 1, 0)\n",
    "if len(test_annotation_df.loc[test_annotation_df['min_bounds_exceeded']==1]) > 0:\n",
    "    print(\"ERROR - example below min example present in data, verify close enough\")\n",
    "    print(test_annotation_df.loc[test_annotation_df['min_bounds_exceeded']==1])\n",
    "if len(test_annotation_df.loc[test_annotation_df['max_bounds_exceeded']==1]) > 0:\n",
    "    print(\"ERROR - example above max example present in data, verify close enough\")\n",
    "    print(test_annotation_df.loc[test_annotation_df['max_bounds_exceeded']==1])\n",
    "\n",
    "#Dropping the start and stop example cols\n",
    "annotation_df = annotation_df.drop(['start_example','stop_example', 'start_example_float','stop_example_float'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 (continued): Now we need to map the wav files in the mapping_filenames.csv back to the images. We do this using the YYMMDDTHHMMS in the image_name to match back to the wav filename's first 12 digits which are also YYMMDD-HHMMS, and joining to get the file lookup number. \n",
    "\n",
    "We don't match on the final 's' value in either filename because there was a batch processing issue with the original files which misrecorded '2' as '4' and will create non-matches between files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column with the image name in a format that is matchable back to the wav file name in the\n",
    "#mapping_filenames.csv file\n",
    "annotation_df['image_name_wav_format'] = (annotation_df['image_name'].str[2:8] + '-' + annotation_df['image_name'].str[9:14])\n",
    "\n",
    "#Getting the matching characters from the mapping_filenames wav names\n",
    "mapping_file_df['wav_name_match'] = mapping_file_df['wav_filename'].str[:12]\n",
    "\n",
    "#Merging the annotation_df and mapping_filenames df together\n",
    "wav_annotation_df = pd.merge(left = annotation_df, right = mapping_file_df, how = 'left',\n",
    "                             left_on='image_name_wav_format', right_on='wav_name_match')\n",
    "\n",
    "#Check that all files have a wav file matching\n",
    "if len(wav_annotation_df.loc[wav_annotation_df['wav_filename'].isnull()]) > 0:\n",
    "    print(\"ERROR - bad merge, some images don't have associated wav files\")\n",
    "\n",
    "#Removing extra columns used to merge datasets\n",
    "wav_annotation_df = wav_annotation_df.drop(['image_name_wav_format','wav_name_match'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 (continued): Now we pull apart the 'example_numbers' column to create one row per example number. We create a final column which shows the mapped_filename-example_number which is the unique label on each vggish point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of sounds and the list of example_numbers\n",
    "sounds_list = list(wav_annotation_df['sound'])\n",
    "image_name_list = list(wav_annotation_df['image_name'])\n",
    "mapped_filename_list = list(wav_annotation_df['mapped_filename'])\n",
    "example_numbers_list = list(wav_annotation_df['example_numbers'])\n",
    "\n",
    "#Creating final table w/ sound, image_name, vggish_point\n",
    "wav_to_annotation_df = pd.DataFrame(columns= ['sound','image_name','vggish_point'])\n",
    "\n",
    "#Iterate through each example numbers array, pull it apart, save each element + the mapped filename and sound ]\n",
    "#to wav_to_annotation_df\n",
    "for i in range(len(example_numbers_list)):\n",
    "    sound = sounds_list[i]\n",
    "    image_name = image_name_list[i]\n",
    "    mapped_filename = mapped_filename_list[i]\n",
    "    examples = example_numbers_list[i]\n",
    "    for example in examples:\n",
    "        vggish_point = str(mapped_filename) + '-' + str(example)\n",
    "        wav_to_annotation_df.loc[len(wav_to_annotation_df)] = [sound, image_name, vggish_point]\n",
    "\n",
    "#Checking that all examples are in the final wav_to_annotation_df\n",
    "example_count = 0\n",
    "for example in example_numbers_list:\n",
    "    example_count = example_count + len(example)\n",
    "if example_count != len(wav_to_annotation_df):\n",
    "    print(\"ERROR - file wrong length. Check code to ensure all examples are included.\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Saving output for later checks\n",
    "#wav_to_annotation_df.to_csv('wav_to_annotation_new_ceil.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Becaue we are using each annotation as the label of the vggish_point in later clustering algorithms we need to ensure each point has a single label. This requires combining labels for vggish_points with multiple annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.21% of points have >1 annotation (1586 out of 10430).\n",
      "Labels have been aggrgated for points with multiple annotations\n"
     ]
    }
   ],
   "source": [
    "#Grouping each point by its annotations to count how many have mult. annotations\n",
    "point_counts = pd.DataFrame(wav_to_annotation_df.groupby(['vggish_point']).count().reset_index())\n",
    "print(\"{0}% of points have >1 annotation ({1} out of {2}).\".format(\n",
    "    round(len(point_counts.loc[point_counts['sound']>1])*100/len(point_counts),2),\n",
    "    len(point_counts.loc[point_counts['sound']>1]), len(point_counts)))\n",
    "\n",
    "#Making vggish_point the primary key and combining sounds into a list per primary key\n",
    "grouped_annotation_df = wav_to_annotation_df.groupby('vggish_point')['sound'].agg(list).reset_index()\n",
    "\n",
    "#Validating we didn't lose any annotations\n",
    "if len(grouped_annotation_df) != len(wav_to_annotation_df['vggish_point'].unique()):\n",
    "    print(\"ERROR - missing points after grouping\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Checking that we captured all vggish_points with > 1 annotation\n",
    "sound_lists = list(grouped_annotation_df['sound'])\n",
    "multi_sound_count = 0\n",
    "for sound in sound_lists:\n",
    "    if len(sound)>1:\n",
    "        multi_sound_count = multi_sound_count + 1\n",
    "    else:\n",
    "        pass\n",
    "if multi_sound_count != len(point_counts.loc[point_counts['sound']>1]):\n",
    "    print(\"ERROR - not all duplicate sounds captured in grouped sound lists\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Iterating through individual points' sounds to remove dupes, alphabetize, and make strings\n",
    "sounds_list = list(grouped_annotation_df['sound'])\n",
    "vggish_point_list = list(grouped_annotation_df['vggish_point'])\n",
    "deduped_labels_df = pd.DataFrame(columns=['vggish_point','label'])\n",
    "for i in range(len(vggish_point_list)):\n",
    "    vggish_point = vggish_point_list[i]\n",
    "    unique_sound_list = []\n",
    "    [unique_sound_list.append(sound) for sound in sounds_list[i] if sound not in unique_sound_list]\n",
    "    unique_sound_list.sort()\n",
    "    label = '-'.join(unique_sound_list)\n",
    "    deduped_labels_df.loc[len(deduped_labels_df)] = [vggish_point, label]\n",
    "print(\"Labels have been aggrgated for points with multiple annotations\")\n",
    "\n",
    "#Validating that there are as many deduplicated points as there were unique points previously\n",
    "if len(grouped_annotation_df) == len(wav_to_annotation_df['vggish_point'].unique()) == len(deduped_labels_df):\n",
    "    pass\n",
    "else:\n",
    "    print(\"ERROR - missing points after deduping annotation labels\")\n",
    "\n",
    "#Validate that the vggish_points are unique\n",
    "if len(deduped_labels_df) != len(list(deduped_labels_df['vggish_point'].unique())):\n",
    "    print(\"ERROR - points are not unique\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 (continued): The vggish_points and their deduplicated annotation labels are saved as vggish_point_Xsec_comb_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the file\n",
    "filename = 'vggish_'+str(example_duration)+'_sec_comb_labels.csv'\n",
    "deduped_labels_df.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggish_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
