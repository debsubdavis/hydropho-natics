# Project Overview

## Problem Statement

Wave energy converters (WEC) transform energy from ocean waves into electricity. Regulatory agencies and scientists are concerned about the noise generated by WECs and its impact on marine life. Researchers have collected underwater audio recordings of WECs but struggle to attribute sounds effectively.

## Data

- **Dataset A:** Recordings from the Fred Olsen WEC from December 1st, 2018, to February 28th, 2019, with 39,746 annotated spectrograms.
- **Dataset B:** Recordings from the Azura WEC from January 6th, 2016, to April 12th, 2016, with 42,338 unannotated spectrograms.

## Model Approach

Utilizing YOLOv8 algorithms, the project aimed to classify sounds in spectrograms, focusing on WEC sounds. 
The "ONE MINUS" method was devised, employing two YOLOv8 models: "ONE" detects all sounds, while "MINUS" identifies specific sounds. The aim was to filter known sounds, reducing unidentified sounds for marine acoustics specialists.

## Methodology

1. **Data Preprocessing:** Utilizing Roboflow, annotations were converted into formats compatible with YOLOv8. Dataset A contained annotations for classes such as Humpback, Flow Noise, Airplane, Helicopter, Boat, and Mooring, while Dataset B had additional classes like Interesting and WEC.

2. **Model Training:** YOLOv8 models were trained on annotated spectrograms from both datasets. The "MINUS" model focused on specific sound categories, while the "ONE" model detected all sounds, regardless of category.

3. **Hyperparameter Tuning:** Extensive tuning was conducted on parameters including epochs, batch size, optimizer type, learning rate, IoU thresholds, and dropout rates. Azure ML's compute clusters facilitated efficient hyperparameter search across a wide range of values.

4. **Training Methodologies:** Both Sequential and Integrated Training methods were explored. Sequential Training began with Dataset A and integrated Dataset B, while Integrated Training involved simultaneous training on both datasets from the outset.

## Model Training

- **Sequential Training:** Initially trained on Dataset A, then integrated Dataset B.

Performance was enhanced by including hyperparameter tuning with combinations of various epochs, batch size, optimizer type, learning rate, IoU thresholds, and dropout rates.

## Results & Conclusion

The "MINUS" model outperformed the "ONE" model, especially when trained solely on Dataset A. Combining datasets reduced performance, indicating Dataset B's complexity. Recommendations include mixing identified and unidentified noises for training and creating a unified model to minimize error margins.

## Interpretation

- Intersection over Union (IoU) analysis showed overlap between model predictions.
- Recommendations include incorporating a mixture of identified and unidentified noises for better training.

For detailed results and images, please refer to the project documentation.


# Guide to Using Project Files

This guide provides detailed instructions on how to use the model template file, hyperparameter tuning file, and Intersection over Union (IoU) framework for your project.

## 1. Model Template File - Yolov8_ModelTemplate.ipynb

The model template file serves as a foundational framework for implementing object detection models, specifically YOLOv8, for sound detection in spectrograms.

### Usage Instructions:

1. **Download the Model Template File:** Obtain the model template file from the designated repository or source provided by the project team.

2. **Customization:** Customize the model template file according to your project requirements. This may include modifying model architecture, input/output configurations, loss functions, and evaluation metrics.

3. **Data Preparation:** Ensure that your dataset is appropriately formatted and preprocessed to align with the input requirements specified in the model template file. The data for 

4. **Training:** Utilize the model template file to train your object detection models on annotated spectrograms. Follow the instructions provided within the file to initiate training and monitor model performance.

5. **Evaluation:** After training, evaluate the performance of your models using relevant evaluation metrics.

6. **Fine-tuning and Iteration:** Iterate on model training and fine-tuning based on evaluation results and feedback to improve model accuracy and performance. 

You can use this model template to train a stand-alone model. The .ipynb file can be run either locally or on Google Colaboratory.

## 2. Hyperparameter Tuning File

The hyperparameter tuning file is designed to streamline the process of optimizing model hyperparameters for improved performance and efficiency.

### Usage Instructions:

1. **Download the Hyperparameter Tuning File:** Obtain the hyperparameter tuning file from the designated repository or source provided by the project team.

2. **Configuration:** Configure the hyperparameter tuning file to specify the range of hyperparameters to be explored during the tuning process. This may include parameters such as epochs, batch size, learning rate, optimizer type, and dropout rates.

3. **Execution:** Execute the hyperparameter tuning file, leveraging the designated computing resources available for the tuning process. Monitor the progress of hyperparameter optimization and adjust configurations as necessary.

4. **Evaluation:** Evaluate the performance of the trained models generated through hyperparameter tuning using appropriate evaluation metrics. Compare the performance of tuned models against baseline models to assess improvements.

5. **Documentation:** Document the hyperparameter tuning process, including configurations, results, and insights gained. This documentation will aid in reproducibility and future iterations of model development.

This notebook can be used to run standalone iterations of hyperparameters locally or on Google colaboratory. The team utilized resources from Azure ML to deploy multiple experiments in parallel using clusters. The Azure setup instructions can be found on the Azure Portal. The team has included the codes that were used to run the parallel hyperparameter tuning experiments.

## 3. Intersection over Union (IoU) Framework

The Intersection over Union (IoU) framework facilitates the analysis of model predictions by calculating the overlap between predicted bounding boxes.

### Usage Instructions:

1. **Download the IoU Framework:** Obtain the IoU framework file from the designated repository or source provided by the project team.

2. **Interpretation:** Interpret IoU scores to assess the accuracy and alignment of predicted bounding boxes with ground truth annotations. Higher IoU scores indicate greater alignment and accuracy of predictions.

3. **Visualization:** Visualize IoU scores and corresponding bounding boxes to gain insights into model performance and areas for improvement.

### File Usage

To use the object detection pipeline, follow these steps:

1. **Specify Input Image**: Provide the path to the test image you want to perform object detection on.

2. **Define Output Directory**: Specify the directory path where you want to save the output files and results.

3. **Set Pretrained Weights**: Define pretrained weights for each model type you want to use in the pipeline.

4. **Execute the Pipeline**: Run the `main` function to execute the object detection pipeline.

Here's an example of how you can execute the pipeline:

```python
if __name__ == "__main__":
    main()
   ```
    

### Expected Input

The object detection pipeline expects the following input:

- **Test Image**: Path to the test image for inference.
- **Output Directory**: Directory path for saving output files.
- **Pretrained Weight Dictionary**: Dictionary containing model types as keys and pretrained weights as values.
- **Image Name**: Name of the image file.

## Expected Output

The object detection pipeline generates the following output:

- **Predictions**: Predictions generated from the one and the minus object detection models.
- **Overlapping Bounding Boxes**: Identification of overlapping bounding boxes of the one and the minus models to assess sound detection.
- **Areas of Interest**: Identification of areas of interest based on the specified Intersection over Union (IoU) threshold. The regions in the spectrogram that have little to no overlap can be further explored as areas of interest to identify WEC and other ambient noises.
- **Plots of Areas of Interest**: Visual representation of areas of interest overlaid on the input image.
- **CSV Files**: Files containing areas of interest data in CSV format. This contains the bounding boxes of individual model predictions as well as the bounding boxes of areas of interest with their coordinates.

## How to Interpret Data

Here's how to interpret the data generated by the object detection pipeline:

1. **Combined Predictions**:
   - This is the predictions of the "One" and the "Minus" models generated individually

2. **Overlapping Bounding Boxes**:
   - Identify overlapping bounding boxes between different models (One and Minus).
   - Assess the consistency and alignment of detections across different models.

3. **Areas of Interest**:
   - Analyze areas of interest based on the specified IoU threshold.
   - Focus on areas where multiple models agree on the presence of objects.

4. **Plots of Areas of Interest**:
   - Visualize areas of interest overlaid on the input image.
   - Use the plots to gain insights into the distribution and location of detected objects.

5. **CSV Files**:
   - Use CSV files containing areas of interest data for further analysis and reporting.
   - Explore the data to understand patterns and correlations in object detections.

By following these guidelines, users can effectively interpret the data generated by the object detection pipeline and derive meaningful insights from the object detection process.

## Input and Output Structure

??

# Conclusion

In conclusion, the object detection pipeline presented in this guide offers a comprehensive framework for performing object detection tasks using multiple models and analyzing the results effectively. By leveraging the YOLOv8 model and Intersection over Union (IoU) framework, users can achieve accurate and consistent object detection across various scenarios.
